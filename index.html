<!DOCTYPE html>
<html lang="en">
<head>
<script>
document.addEventListener("DOMContentLoaded", function() {
    const seeMoreLink = document.querySelector(".see-more");
    const moreUpdates = document.querySelector(".more-updates");

    seeMoreLink.addEventListener("click", function(event) {
        event.preventDefault();
        if (moreUpdates.style.display === "none") {
            moreUpdates.style.display = "block";
            seeMoreLink.textContent = "See less";
        } else {
            moreUpdates.style.display = "none";
            seeMoreLink.textContent = "See more";
        }
    });
});
</script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daniela Massiceti - Machine Learning Researcher</title>
    <link rel="stylesheet" href="static/css/main.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <div class="side-panel">
            <section class="profile-section">
                <img src="static/images/profile-photo.png" alt="Daniela Massiceti" class="profile-photo">
                <h1>Daniela Massiceti</h1>
                <p class="title">Senior Researcher at Microsoft Research</p>
            </section>
            <section class="introduction">        
                <p>Hello! I am a senior machine learning researcher at <a href="https://www.microsoft.com/en-us/research/people/dmassiceti/">Microsoft Research</a>, based in Sydney.</p>
                <p>I have over a decade of experience in multimodal AI, particularly in image and text domains, and a passion for doing multi-disciplinary research that tackles complex sociotechnical challenges.</p>
                <p>My current focus areas include how we incorporate new knowledge into multi-modal generative models (e.g. diffusion models, MLLMs), and how we evaluate these models more broadly.</p>
                <p>I did my PhD in Computer Vision (under Prof. Philip Torr) and M.Sc Neuroscience at the University of Oxford, and my B.Sc in Electrical and Computer Engineering at the University of Cape Town. I am also a long-time organiser of the <a href="https://deeplearningindaba.com" target="_blank">Deep Learning Indaba</a>.</p>
            </section>
        </div>
        <main class="main-panel">
            <div class="social-buttons">
                <a href="https://scholar.google.com/citations?user=-4fo-SwAAAAJ&hl=en" target="_blank" class="icon-button scholar"><i class="fas fa-graduation-cap"></i></a>
                <a href="http://za.linkedin.com/pub/daniela-massiceti/66/422/425" target="_blank" class="icon-button linkedin"><i class="fab fa-linkedin"></i></a>
                <a href="https://github.com/danielamassiceti" target="_blank" class="icon-button github"><i class="fab fa-github"></i></a>
                <a href="https://www.twitter.com/DanniMassi/" target="_blank" class="icon-button twitter"><i class="fab fa-twitter"></i></a>
                <a href="mailto:daniela.massiceti@gmail.com" class="icon-button email"><i class="fas fa-envelope"></i></a>
            </div>
            <section class="news-updates">
                <h2 class="section-title">Recent News</h2>
                <ul>
                    <li><b>Aug, 2025:</b> I served as an Area Chair for the <a href="https://neurips.cc/Conferences/2025/CallForDatasetsBenchmarks">NeurIPS Datasets and Benchmarks Track 2025</a>.</li>
                    <li><b>Aug, 2025:</b> Our paper "Investigating Dictionary Expansion for Video-based Sign Language Dictionaries" was accepted at EMNLP 2025.</li>
                    <li><b>Oct, 2024:</b> I was interviewed about <i>Find My Things</i> on the <a href="https://www.microsoft.com/en-us/research/podcast/abstracts-september-30-2024/">Microsoft Research podcast</a>, alongside Martin Grayson.</li>
                    <li><b>Sep, 2024:</b> <i>Find My Things</i> was honoured by the Fast Company in the <a href="https://www.fastcompany.com/91128700/accessible-design-innovation-by-design-2024" target="_blank">Accessible Design</a> and <a href="https://www.fastcompany.com/91129207/artificial-intelligence-innovation-by-design-2024" target="_blank">Artificial Intelligence</a> categories in their 2024 Innovation by Design Awards.</li>
                    <li><b>Sep, 2024:</b> Our paper <a href="https://arxiv.org/abs/2406.04236" target="_blank">"Understanding Information Storage and Transfer in Multi-modal Large Language Models"</a> was accepted at NeurIPS 2024.</li>
                    <li><b>Sep, 2024:</b> Our paper <a href="https://arxiv.org/abs/2307.09233" target="_blank">"Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP"</a> was accepted at EMNLP 2024.</li>
                    <li><b>Jun, 2024:</b> I gave a talk on <a href="https://researchforum.microsoft.com/Home/SessionDetail?sessionId=bff28fd8-2ab7-4773-a846-8dc456522add&compId=dcba818b-7de7-4055-a5c1-e01fbab57e4a&releaseId=cbcb3c19-ce62-4ac7-99af-1ee6b9980f58&showFromPaylod=true" target="_blank">CLIP's performance gaps for blind/low vision users</a>, and participated in a <a href="https://researchforum.microsoft.com/Home/SessionDetail?sessionId=48b2ee01-47cf-42e7-837d-283f326eb2b8&compId=dcba818b-7de7-4055-a5c1-e01fbab57e4a&releaseId=cbcb3c19-ce62-4ac7-99af-1ee6b9980f58&showFromPaylod=true" target="_blank">panel on Equitable AI</a> at the <a href="https://researchforum.microsoft.com/" target="_blank">Microsoft Research Forum</a>.</li>
                    <li><b>June, 2024:</b> I co-organised the <a href="https://vizwiz.org/workshops/2024-vizwiz-grand-challenge-workshop/" target="_blank">6th VizWiz Grand Challenge</a> workshop at CVPR 2024 in Seattle.</li>
                    <li><b>Apr, 2024:</b> <i>Find My Things</i>, a personalisable object recogniser, was shipped as a new feature in Microsoft's Seeing AI app - the culmination of over 3 years' work! </li>
                    <li><b>Feb, 2024:</b> Our paper <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Massiceti_Explaining_CLIPs_Performance_Disparities_on_Data_from_BlindLow_Vision_Users_CVPR_2024_paper.html" target="_blank">"Explaining CLIP's performance disparities on data from blind/low vision users"</a> was accepted at CVPR 2024.</li>
                    <li><b>Dec, 2023:</b> Our paper <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28978" target="_blank">"Strong Baselines for Parameter-Efficient Few-Shot Fine-Tuning"</a> was accepted at AAAI 2024.</li>
                    <li><b>Jun, 2023:</b> I co-organised the <a href="https://vizwiz.org/workshops/2023-workshop/" target="_blank">5th VizWiz Grand Challenge</a> workshop at CVPR 2023 in Vancouver.</li>
                    <li><b>Feb, 2023:</b> Our paper <a href="https://openreview.net/forum?id=wq0luyH3m4" target="_blank">HardMD++: Towards Understanding Few-Shot Performance on Difficult Tasks</a> was accepted at ICLR 2023.</li>
                    <li><b>Aug, 2022:</b> I've moved to Sydney, Australia with MSR!</li>
                    <div class="more-updates" style="display: none;">
                        <li><b>Jul, 2022:</b> We launched the <a href="https://open.spotify.com/show/7AsiLuLq1Ay7QMBOUJNHfu" target="_blank">VizWiz podcast</a>, featuring discussions with computer vision researchers, blind technology advocates, and accessibility industry specialists.</li>
                        <li><b>Jun, 2022:</b> I co-organised the <a href="https://vizwiz.org/workshops/2022-workshop/" target="_blank">4th VizWiz Grand Challenge</a> workshop at CVPR 2022 in New Orleans.</li>
                        <li><b>May, 2022:</b> I won the <i>Innovating for the Future Award</i> at the <a href="https://news.microsoft.com/ability-summit-2022/" target="_blank">Microsoft Ability Summit 2022</a>.</li>
                        <li><b>Feb, 2022:</b> We launched the <a href="https://eval.ai/web/challenges/challenge-page/1438/overview" target="_blank">ORBIT Few-Shot Object Recognition Challenge</a> which will be hosted at the <a href="https://vizwiz.org/workshops/2022-workshop" target="_blank">VizWiz Grand Challenge</a> workshop at CVPR 2022</a>.</li>
                        <li><b>Dec, 2021:</b> I was invited to a <a href="https://www.microsoft.com/en-us/research/video/panel-2-pursuing-a-resilient-and-sustainable-global-society/" target="_blank">panel discussion on <i>Pursuing a Resilient and Sustainable Global Society</i></a> as part of Microsoft Research's 30th Anniversary Panel Series on Inspirational and Impactful Research.</li>
                        <li><b>Dec, 2021:</b> I gave a <a href="https://sites.google.com/view/wiml2021/program" target="_blank">talk on the ORBIT Dataset</a> at the <a href="https://sites.google.com/view/wiml2021/home" target="_blank">Women in Machine Learning (WiML) workshop</a> at NeurIPS 2021.</li>
                        <li><b>Oct, 2021:</b> Our team won <a href="https://www.linkedin.com/posts/daniela-massiceti-42542266_microsoft-global-hackathon-2021-award-winner-activity-6980785974683332608-bFhv" target="_blank">1st prize in Microsoft's internal global hackathon</a> out of over 6000 teams!</li>
                        <li><b>Oct, 2021:</b> I gave a <a href="https://www.youtube.com/watch?v=WpvOkIJjg-A" target="_blank">talk at the Microsoft Research Summit</a> on how few-shot learning can be used to personalise AI systems to users.</li>
                        <li><b>Oct, 2021:</b> We reached 200 mentorship sessions in the Indaba Mentorship Programme! If you're interested in mentoring African ML students, sign-up as a mentor <a href="https://deeplearningindaba.com/mentorship/" target="_blank">here</a>.</li>
                        <li><b>Oct, 2021:</b> I joined the <a href="https://vizwiz.org" target="_blank">VizWiz Grand Challenge</a> organising committee, an annual workshop working to improve AI technologies for the blind/low vision community.</li>
                        <li><b>Sep, 2021:</b> Our paper <a href="https://proceedings.neurips.cc/paper/2021/hash/cc1aa436277138f61cda703991069eaf-Abstract.html" target="_blank">"Memory Efficient Meta-Learning with Large Images"</a> was accepted at NeurIPS 2021.</li>
                        <li><b>Jul, 2021:</b> Our paper <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Massiceti_ORBIT_A_Real-World_Few-Shot_Dataset_for_Teachable_Object_Recognition_ICCV_2021_paper.html" target="_blank">"ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition"</a> was accepted at ICCV 2021.</li>
                        <li><b>Jul, 2021:</b> Our paper <a href="https://dl.acm.org/doi/10.1145/3441852.3471225" target="_blank">"Disability-first Dataset Creation: Lessons from Constructing a Dataset for Teachable Object Recognition with Blind and Low Vision Data Collectors"</a> was accepted at ASSETS 2021.</li>
                        <li><b>Jun, 2021:</b> I gave a talk on the ORBIT dataset</a> at the <a href="https://vizwiz.org/workshops/2021-workshop" target="_blank">CVPR 2021 VizWiz Grand Challenge workshop</a>.</li>
                        <li><b>Apr, 2021:</b> I wrapped up 12 months of languishing away in a global pandemic.</li>
                        <li><b>Mar, 2021:</b> I was "upgraded" to Senior Researcher at MSR (and also moved to Cambridge)!</li>
                        <li><b>Feb, 2021:</b> I gave a guest lecture on dataset bias</a> at the Department of Computer Science and Technology, University of Cambridge.</li>
                        <li><b>Jan, 2021:</b> I co-launched the <a href="https://deeplearningindaba.com/mentorship" target="_blank">Deep Learning Indaba Mentorship Programme</a> alongside an excellent team including <a href="https://www.linkedin.com/in/avishkarbhoopchand/?originalSubdomain=uk" target="_blank">Aviskhar Bhoopchand</a>, <a href="https://www.linkedin.com/in/siobhan-mackenzie-hall-805255bb/?originalSubdomain=uk" target="_blank">Siobhan Hall</a>, and <a href="https://www.linkedin.com/in/kale-ab-tessera-013976101/?originalSubdomain=za" target="_blank">Kale-ab Tessera</a>.</li>
                        <li><b>May, 2020:</b> I launched the <a href="https://orbit.city.ac.uk" target="_blank">ORBIT Project</a>, am effort to collect data with blind/low vision users, alongside an amazing team of collaborators.</li>
                        <li><b>Apr, 2020:</b> I was appointed as a <a href="https://www.st-edmunds.cam.ac.uk/">St. Edmund's College Cambridge</a> Postdoctoral Research Fellow.</li>
                        <li><b>Feb, 2020:</b> I started as a Researcher at Microsoft Research Cambridge.</li>
                        <li><b>Oct, 2019:</b> I defended my <a href="https://ora.ox.ac.uk/objects/uuid:c004de16-0b4a-4d84-899a-040c8d07775e" target="_blank">D.Phil thesis</a> and passed! My examiners were <a href="https://www.robots.ox.ac.uk/~az/" target="_blank">Prof. Andrew Zisserman</a> and <a href="http://www.cs.utexas.edu/users/grauman/" target="_blank">Prof. Kristen Grauman</a>.</li>
                        <li><b>Sept, 2019:</b> We wrapped up the 3rd <a href="https://deeplearningindaba.com" target="_blank">Deep Learning Indaba</a> in Nairobi Kenya. I coordinated the <a href="https://deeplearningindaba.com/blog/2019/07/outcomes-of-2019-kambule-and-maathai-awards/" target="_blank">Kambule and Maathai Awards</a> (with <a href="https://adjidieng.github.io/" target="_blank">Adji Bousso Dieng</a>), a <i>How to apply for a PhD</i> workshop (with <a href="https://laurasevilla.me/" target="_blank">Laura Sevilla-Lara</a>, <a href="https://cs.brown.edu/~gdk/" target="_blank">George Konidaris</a>), and a computer vision parallel track (with <a href="https://laurasevilla.me/" target="_blank">Laura Sevilla-Lara</a>)</li>
                        <li><b>Aug, 2019:</b> I finished a 3-month internship at Microsoft Research Cambridge. I worked with <a href="https://www.microsoft.com/en-us/research/people/cecilym/" target="_blank">Cecily Morrison</a>, <a href="https://www.tschiatschek.net/" target="_blank">Sebastian Tschiatschek</a> and <a href="https://khofm.wordpress.com/" target="_blank">Katja Hofmann</a>.</li>
                        <li><b>May, 2019:</b> I submitted my D.Phil thesis <i><a href="https://ora.ox.ac.uk/objects/uuid:c004de16-0b4a-4d84-899a-040c8d07775e" target="_blank">"Computer vision and natural language processing for people with vision impairment"</a></i>.</li>
                    </div>
                </ul>
                <a href="#" class="see-more">See more updates</a>
            </section>
            <section class="publications">
                <h2 class="section-title">Selected Publications</h2>
                <p>*See my <a href="https://scholar.google.com/citations?user=-4fo-SwAAAAJ&hl=en" target="_blank">Google Scholar</a> for a complete list.</p>
                <div class="publication">
                    <h3>Understanding Information Storage and Transfer in Multi-modal Large Language Models (NeurIPS 2024)</h3>
                    <p class="authors">Samyadeep Basu, Martin Grayson, Cecily Morrison, Besmira Nushi, Soheil Feizi, <u>Daniela Massiceti</u></p>
                   <p class="tldr"><strong>TLDR:</strong> We introduce a causality-based framework to study how multi-modal models store and transfer information in VQA tasks.</p>
                    <a href="https://arxiv.org/abs/2406.04236" target="_blank" class="button-link">Paper</a>
                    <a href="" target="_blank" class="button-link">Code (coming soon)</a>
                </div>
                <div class="publication">
                    <h3>Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP (EMNLP 2024)</h3>
                    <p class="authors">Samyadeep Basu, Maziar Sanjabi, <u>Daniela Massiceti</u>, Shell Xu Hu, Soheil Feizi</p>
                    <p class="tldr"><strong>TLDR:</strong> We propose a novel loss function for training CLIP which distills spatial reasoning abilities from a text-to-image model.</p>
                    <a href="https://arxiv.org/abs/2307.09233" target="_blank" class="button-link">Paper</a>
                </div>
                <div class="publication">
                    <h3>Explaining CLIP's performance disparities on data from blind/low vision users (CVPR 2024)</h3>
                    <p class="authors"><u>Daniela Massiceti</u>, Camilla Longden, Agnieszka S&#322;owik, Samuel Wills, Martin Grayson, Cecily Morrison</p>
                    <p class="tldr"><strong>TLDR:</strong> We systematically evaluate CLIP on image and text data captured by blind/low vision users and reveal significant performance gaps.</p>
                    <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Massiceti_Explaining_CLIPs_Performance_Disparities_on_Data_from_BlindLow_Vision_Users_CVPR_2024_paper.html" target="_blank" class="button-link">Paper</a>
                    <a href="https://www.youtube.com/watch?v=Iu-NB5b5Xlo" target="_blank" class="button-link">Video</a>
                    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30937.png?t=1717034018.9350114" target="_blank" class="button-link">Poster</a>
                </div>
                <div class="publication">
                    <h3>Strong Baselines for Parameter-Efficient Few-Shot Fine-Tuning (AAAI 2024)</h3>
                    <p class="authors">Samyadeep Basu, <u>Daniela Massiceti</u>, Shell Xu Hu, Soheil Feizi</p>
                    <p class="tldr"><strong>TLDR:</strong> We introduce two simple baselines for parameter-efficient fine-tuning a Vision Transformer for a few-shot image classification task.</p>
                    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28978" target="_blank" class="button-link">Paper</a>
                </div>
                <div class="publication">
                    <h3>HardMD++: Towards Understanding Few-Shot Performance on Difficult Tasks (ICLR 2023)</h3>
                    <p class="authors">Samyadeep Basu and Megan Stanley and John F Bronskill and Soheil Feizi, <u>Daniela Massiceti</u></p>
                    <p class="tldr"><strong>TLDR:</strong> We introduce HardMetaDataset++, a new few-shot image classification benchmark for understanding performance on difficult tasks.</p>
                    <a href="https://openreview.net/forum?id=wq0luyH3m4" target="_blank" class="button-link">Paper</a>
                    <a href="https://github.com/Samyadeep/HardMD" target="_blank" class="button-link">Code</a>
                    <a href="https://iclr.cc/virtual/2023/poster/11540" target="_blank" class="button-link">Video</a>
                    <a href="https://iclr.cc/media/PosterPDFs/ICLR%202023/11540.png?t=1682623241.3373032" target="_blank" class="button-link">Poster</a>
                </div>
                <div class="publication">
                    <h3>Understanding Personalized Accessibility through Teachable AI: Designing and Evaluating Find My Things for People who are Blind or Low Vision (ASSETS 2023)</h3>
                    <p class="authors">Cecily Morrison, Martin Grayson, Rita Faia Marques, <u>Daniela Massiceti</u>, Camilla Longden, Linda Wen, Edward Cutrell</p>
                    <p class="tldr"><strong>TLDR:</strong> We describe the development and evaluation of <i>Find My Things</i>, a personalisable object recogniser for people who are blind/low vision.</p>
                    <a href="https://dl.acm.org/doi/abs/10.1145/3597638.3608395" target="_blank" class="button-link">Paper</a>
                </div>
                <div class="publication">
                    <h3>Memory Efficient Meta-Learning with Large Images (NeurIPS 2021)</h3>
                    <p class="authors">John Bronskill*, <u>Daniela Massiceti</u>*, Massimiliano Patacchiola*, Katja Hofmann, Sebastian Nowozin, Richard E. Turner</p>
                    <p class="tldr"><strong>TLDR:</strong> We introduce a memory-efficient algorithm called LITE for meta-learning a few-shot image classification task with large images.</p>
                    <a href="https://proceedings.neurips.cc/paper/2021/hash/cc1aa436277138f61cda703991069eaf-Abstract.html" target="_blank" class="button-link">Paper</a>
                    <a href="https://github.com/microsoft/ORBIT-Dataset" target="_blank" class="button-link">Code (ORBIT)</a>
                    <a href="https://github.com/cambridge-mlg/LITE" target="_blank" class="button-link">Code (VTAB+MD)</a>
                    <a href="https://neurips.cc/virtual/2021/poster/27033" target="_blank" class="button-link">Poster</a>
                </div>
                <div class="publication">
                    <h3>ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition (ICCV 2021)</h3>
                    <p class="authors"><u>Daniela Massiceti</u>, Luisa Zintgraf, John Bronskill, Lida Theodorou, Matthew Tobias Harris, Edward Cutrell, Cecily Morrison, Katja Hofmann, Simone Stumpf</p>
                    <p class="tldr"><strong>TLDR:</strong> We introduce challenging dataset of videos taken by blind/low vision users of their personal objects, and a new few-shot object recognition benchmark.</p>
                    <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Massiceti_ORBIT_A_Real-World_Few-Shot_Dataset_for_Teachable_Object_Recognition_ICCV_2021_paper.html" target="_blank" class="button-link">Paper</a>
                    <a href="https://github.com/microsoft/ORBIT-Dataset" target="_blank" class="button-link">Code</a>
                    <a href="https://city.figshare.com/articles/dataset/ORBIT_A_real-world_few-shot_dataset_for_teachable_object_recognition_collected_from_people_who_are_blind_or_low_vision/14294597" target="_blank" class="button-link">Dataset</a>
                </div>
            </section>
        </main>
    </div>
    <footer class="footer">
        <p>&copy; 2024 Daniela Massiceti. All rights reserved.</p>
    </footer>
</body>
</html>