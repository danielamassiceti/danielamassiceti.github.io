<!doctype html>
<html>
<head id="head">
<meta charset="UTF-8">
<script src="/static/global_vars.js"></script>
<title>daniela massiceti // orbit</title>

<link rel="stylesheet" type="text/css" href="/static/css/main.css" media="screen"/>
<link rel="stylesheet" type="text/css" href="/static/css/twitter-bootstrap.css" />
<link rel="shortcut icon" href="/static/images/tabby.ico" >
<link rel="icon" href="/static/images/tabby.ico">
<link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Comfortaa" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-F3MF0CTQF7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-F3MF0CTQF7');
</script>

<meta name="description" content="daniela massiceti" />
<meta name="p:domain_verify" content="15f149f4e12ce0bf04ad541a6e0d8a62"/>
</head>
<script>
  document.getElementById("head").innerHTML += '<meta name="keywords" content=' + mykeywords + '/>';
</script>

<body>
<div id="wrapper">
  <header id="top">
    <a href="https://github.com/danielamassiceti" target=_blank><img id="github" src="/static/socialmediaicons/github.png" width="30" height="30" title="github" alt="github"/></a>
    <a href="https://scholar.google.com/citations?user=-4fo-SwAAAAJ&hl=en" target=_blank><img id="googlescholar" src="/static/socialmediaicons/gscholar.png" width="30" height="30" title="google scholar" alt="google scholar"/></a>
    <a href="http://za.linkedin.com/pub/daniela-massiceti/66/422/425" target=_blank><img id="linkedin" src="/static/socialmediaicons/linkedin.png" width="30" height="30" title="linkedin" alt="linkedin"/></a>
    <a href="mailto:daniela.massiceti@gmail.com" target="_top"><img id="mail" src="/static/socialmediaicons/mail_newcolour.png" width="30" height="30" title="mail: daniela.massiceti@gmail.com" alt="mail"/></a>
    <a href="https://www.twitter.com/DanniMassi/" target=_blank><img id="twitter" src="/static/socialmediaicons/twitter.png" width="30" height="30" title="twitter: @dannimassi" alt="twitter"/></a>
  </header>
   
  <div id="headerimg">
    <a href="/index.html">
      <div class = "h">Daniela Massiceti</div>
    </a>
    <div class="subh" id="jobtitle"></div>
      <script>document.getElementById("jobtitle").innerHTML = jobtitle;</script>
  </div>
 	<hr id="tophr"> 
	    <nav class="navigationbar" id="extendedbar">
    		<ul>
          	<li><a class="normal" href="/index.html" id=" ">home</a></li>
          	<li><a class="normal" href="/about.html" id=" ">about</a></li>
          	<li><a class="normal" href="/research.html" id=" ">research</a> <a class="normal" href="/research/orbit_iccv/orbit_iccv.html" id="currentPage">&middot orbit dataset</a></li>
          	<li><a id="cv" href="" target=_blank id=" ">cv</a></li>
	    	  <script>document.getElementById("cv").href = cvpath;</script>
    		</ul>
      </nav>
  <hr id="demo">
  <div id="mainbody">
    <div id="titlebox">
      <h1>ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition</h1>
      <h2>Daniela Massiceti, Luisa Zintgraf, John Bronskill, Lida Theodorou, Matthew Tobias Harris, Edward Cutrell, Cecily Morrison, Katja Hofmann, Simone Stumpf</h2><br>
    </div>

    <div class="abstract">	
	    Object recognition has made great advances in the last decade, but predominately still relies on many high-quality training examples per object category. In contrast, learning new objects from only a few examples could enable many impactful applications from robotics to user personalization. Most few-shot learning research, however, has been driven by benchmark datasets that lack the high variation that these applications will face when deployed in the real-world. To close this gap, we present the ORBIT dataset and benchmark, grounded in a real-world application of teachable object recognizers for people who are blind/low-vision. The dataset contains 3,822 videos of 486 objects recorded by people who are blind/low-vision on their mobile phones, and the benchmark reflects a realistic, highly challenging recognition problem, providing a rich playground to drive research in robustness to few-shot, high-variation conditions. We set the first state-of-the-art on the benchmark and show that there is massive scope for further innovation, holding the potential to impact a broad range of real-world vision applications including tools for the blind/low-vision community.
    </div>
    <video class="abstractimg" width="50%" autoplay loop>
      <source src="/research/orbit_iccv/static/hook.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
	
    <hr id="tophr2">
    <nav class="navigationbar">
      <ul>
        <li><a class="normal" href="/research/orbit_iccv/orbit_iccv.html" id="currentPage">abstract</a></li>
        <li><a class="normal" href="https://arxiv.org/abs/2104.03841" target="_blank">paper</a></li>
	<li><a class="normal" href="https://github.com/microsoft/ORBIT-Dataset" target="_blank">code</a></li>
	<li><a class="normal" href="https://city.figshare.com/articles/dataset/_/14294597" target="_blank">dataset</a></li>
      </ul>
    </nav>
    <hr id="bottomhr2">

  	<div id="footerwrapper">
		  <div id="email"> daniela_dot_massiceti_at_gmail_dot_com </div>
      <img src="/static/images/robotgirl.png" alt=""/>
	  </div>
  </div>
</div>
</body>
</html>
